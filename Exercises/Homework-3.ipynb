{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 1: Parameter Counting and Memory Management (30 points)\n",
        "\n",
        "Show your work for the following short-answer questions.\n",
        "\n",
        "## part 1(a) Parameter counting (10 points)\n",
        "\n",
        "A more advanced version of the CNN we trained may have three convolutional layers, each with $3 \\times 3$ kernels, a stride of 2, and \"same\" padding.\n",
        "The lowest (first) `Conv2D` layer outputs 100 feature maps, the middle layer outputs 200, and the top (last) one outputs 400 maps.\n",
        "These operate on input images that are RGB images of $200 \\times 300$ pixels.\n",
        "\n",
        "Show that this network will comprise more than 800,000 parameters.\n",
        "\n",
        "## part 1(b) Memory management (10 points)\n",
        "\n",
        "The parameters in part 1(a) are typically represented using 32-bit floating point numbers.\n",
        "Show that this network will require at least 12 MB of RAM when making a *prediction* for a single data instance (input image).\n",
        "\n",
        "## part 1(c) Memory management in training(5 points)\n",
        "\n",
        "How much RAM is needed if mini-batch *training* is used with batches of 50 images? (*Hint*: it is at least 500 MB.)\n",
        "\n",
        "## part 1(d) RNN dimensions (5 points)\n",
        "\n",
        "The input to an RNN layer must have three dimensions.\n",
        "What does each input dimension represent?\n",
        "And what do the outputs of the RNN layer represent?"
      ],
      "metadata": {
        "id": "RCW83RcF1YIx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 2: CNN Training for Quantum State Classification (35 points)\n",
        "\n",
        "In this exercise you will train a CNN on a dataset of quantum states for a particle in a 1-d box of length $L$.\n",
        "\n",
        "The goal is to identify the quantum index of the eigenfunction: $n=1,2,3,\\ldots$, or the dominant eigenfunction for a mixed state.\n",
        "\n",
        "*Bonus question: why is the CNN a better choice for this challenge than a RNN?*\n",
        "\n"
      ],
      "metadata": {
        "id": "2-bhbSUzI8xh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate dataset\n",
        "\n",
        "We give you the dataset generator so that everyone has a similar dataset.\n",
        "\n",
        "The basic spatial eigenfunction is\n",
        "$\\psi(x) = \\sqrt{\\frac{2}{L}} \\sin(n\\pi x/L),$\n",
        "but that is too simple to learn, so we add some Gaussian noise to the function to make it challenging."
      ],
      "metadata": {
        "id": "lAGRmd-eVW9_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {DEVICE}\")\n",
        "\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Parameters for the simulation\n",
        "NUM_POINTS = 256          # points in the spatial function\n",
        "NUM_CLASSES = 5           # quantum numbers n = 1 through 5\n",
        "NOISE_STD = 0.3               # std of Gaussian noise to add\n",
        "\n",
        "def generate_noisy_pinb(n_samples, noise_std=NOISE_STD, L=1.0):\n",
        "    \"\"\"Generate noisy wavefunctions for Particle In Box\n",
        "       with known box length L.\"\"\"\n",
        "    x = np.linspace(0, L, NUM_POINTS, endpoint=False)\n",
        "    X = np.zeros((n_samples, NUM_POINTS))\n",
        "    y = np.zeros(n_samples, dtype=int)\n",
        "    for i in range(n_samples):\n",
        "        n = np.random.randint(1, NUM_CLASSES + 1)          # n = 1…5\n",
        "        psi = np.sqrt(2 / L) * np.sin(n * np.pi * x / L)\n",
        "        X[i] = psi + np.random.normal(0, noise_std, NUM_POINTS)\n",
        "        y[i] = n - 1                                       # class index 0…4\n",
        "    return X, y\n",
        "\n",
        "def make_data_loaders(generate_fn, n_train=5000, n_val=1000, batch_size=64):\n",
        "    \"\"\"Generate data with some `generate_fn` and wrap in DataLoaders.\"\"\"\n",
        "    X_train, y_train = generate_fn(n_train)\n",
        "    X_val,   y_val   = generate_fn(n_val)\n",
        "\n",
        "    # Reshape to (N, 1, NUM_POINTS) for Conv1d\n",
        "    X_train = torch.tensor(X_train, dtype=torch.float32).unsqueeze(1)\n",
        "    y_train = torch.tensor(y_train, dtype=torch.long)\n",
        "    X_val   = torch.tensor(X_val,   dtype=torch.float32).unsqueeze(1)\n",
        "    y_val   = torch.tensor(y_val,   dtype=torch.long)\n",
        "\n",
        "    train_ds = TensorDataset(X_train, y_train)\n",
        "    val_ds   = TensorDataset(X_val,   y_val)\n",
        "    return (DataLoader(train_ds, batch_size, shuffle=True),\n",
        "            DataLoader(val_ds,   batch_size))\n",
        "\n",
        "train_loader, val_loader = make_data_loaders(generate_noisy_pinb)\n"
      ],
      "metadata": {
        "id": "PMx9fVi9TJrT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## part 2(a): Check the dataset (5 points)\n",
        "\n",
        "Plot the first 5 inputs in the `train_loader` as spatial functions, just to check that they look reasonable."
      ],
      "metadata": {
        "id": "r2JkGcpCTJAV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Look at batch 0 to extract the first few samples\n",
        "for i, (X_batch, y_batch) in enumerate(train_loader):\n",
        "    if i == 0:\n",
        "        # Add your code to plot the first 5 inputs in X_batch and y_batch"
      ],
      "metadata": {
        "id": "11tFyU0lZI6D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## part 2(b): Construct CNN (10 points)\n",
        "\n",
        "The CNN is based on the `Conv1d` object in PyTorch.\n",
        "\n",
        "We have set up the first layer and the third layer.\n",
        "- Now you add the second convolutional layer with a $5\\times 5$ kernel and padding = 2.\n",
        "- Explain why the last convolutional layer uses the *average* pooling method instead of the *maximum* pooling method.\n",
        "- Explain what is happening with the `squeeze` in the `forward` function.\n",
        "\n"
      ],
      "metadata": {
        "id": "ME-OM1ZKZ2Yv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class WavefunctionCNN(nn.Module):\n",
        "    \"\"\"1-D CNN particle-in-a-box wavefunctions.\"\"\"\n",
        "\n",
        "    def __init__(self, num_classes=NUM_CLASSES):\n",
        "        super().__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv1d(1, 16, kernel_size=7, padding=3),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(2),\n",
        "\n",
        "            # Add missing hidden layer here\n",
        "\n",
        "            nn.Conv1d(32, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.AdaptiveAvgPool1d(1),\n",
        "        )\n",
        "        self.classifier = nn.Linear(64, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (batch, 1, NUM_POINTS)\n",
        "        x = self.features(x)\n",
        "        x = x.squeeze(-1)                     # (batch, 64)\n",
        "        return self.classifier(x)\n"
      ],
      "metadata": {
        "id": "odGHRTChWiqx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## part 2(c): Train CNN (5 points)\n",
        "\n",
        "- Pick a reasonable criterion and optimizer for the CNN training and add them to the `train_model` function.\n",
        "- What is the role of the `logits.argmax(1)` call inside the mini-batch training?\n",
        "- Then train the CNN on the data loaders and plot the evolution of the loss and accuracy as a function of epoch.\n",
        "\n"
      ],
      "metadata": {
        "id": "J8lRrlPXcd0g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "def train_model(model, train_loader, val_loader, epochs=20, lr=0.001):\n",
        "    # Add your criterion and optimizer here\n",
        "    criterion =\n",
        "    optimizer =\n",
        "\n",
        "    history = {\"train_loss\": [], \"val_loss\": [], \"train_acc\": [], \"val_acc\": []}\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        model.train()\n",
        "        running_loss, correct, total = 0.0, 0, 0\n",
        "        for X_batch, y_batch in train_loader:\n",
        "            X_batch, y_batch = X_batch.to(DEVICE), y_batch.to(DEVICE)\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(X_batch)\n",
        "            loss = criterion(logits, y_batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item() * X_batch.size(0)\n",
        "            # What does logits.argmax(1) do here?\n",
        "            correct += (logits.argmax(1) == y_batch).sum().item()\n",
        "            total += X_batch.size(0)\n",
        "\n",
        "        history[\"train_loss\"].append(running_loss / total)\n",
        "        history[\"train_acc\"].append(correct / total)\n",
        "\n",
        "        # validation\n",
        "        model.eval()\n",
        "        val_loss, val_correct, val_total = 0.0, 0, 0\n",
        "        with torch.no_grad():\n",
        "            for X_batch, y_batch in val_loader:\n",
        "                X_batch, y_batch = X_batch.to(DEVICE), y_batch.to(DEVICE)\n",
        "                logits = model(X_batch)\n",
        "                loss = criterion(logits, y_batch)\n",
        "                val_loss += loss.item() * X_batch.size(0)\n",
        "                val_correct += (logits.argmax(1) == y_batch).sum().item()\n",
        "                val_total += X_batch.size(0)\n",
        "\n",
        "        history[\"val_loss\"].append(val_loss / val_total)\n",
        "        history[\"val_acc\"].append(val_correct / val_total)\n",
        "\n",
        "        if epoch % 5 == 0 or epoch == 1:\n",
        "            print(f\"Epoch {epoch:3d}/{epochs}  \"\n",
        "                  f\"train_loss={history['train_loss'][-1]:.4f}  \"\n",
        "                  f\"val_acc={history['val_acc'][-1]:.3f}\")\n",
        "\n",
        "    return history"
      ],
      "metadata": {
        "id": "cN4xYDJcS8RO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code here to train the network using the function definitions above.\n",
        "# Use the train_loader and val_loader data.\n",
        "# You can use the history output to make the plots of loss and accuracy\n",
        "train_loader1, val_loader1 = make_data_loaders(generate_noisy_pinb)\n"
      ],
      "metadata": {
        "id": "yEJHXXJpXs9L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## part 2(d) (5 points)\n",
        "\n",
        "The next challenge for the CNN is to identify the different eigenstates even with varied box lengths.\n",
        "\n",
        "To that end, we generate a new dataset with L values varying from 0.5 to 2.0, and we add the same Gaussian noise to each curve.\n",
        "\n",
        "Add code to retrain the CNN using these new data and evaluate the performance. How do the results compare to the results from part 2(c)?"
      ],
      "metadata": {
        "id": "x0zoe6BPS2yc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_varying_L(n_samples, L_range=(0.5, 2.0), noise_std=0.3):\n",
        "    \"\"\"Generate wavefunctions with random box length L.\"\"\"\n",
        "    X = np.zeros((n_samples, NUM_POINTS))\n",
        "    y = np.zeros(n_samples, dtype=int)\n",
        "    for i in range(n_samples):\n",
        "        L = np.random.uniform(*L_range)\n",
        "        n = np.random.randint(1, NUM_CLASSES + 1)\n",
        "        # We always sample x in [0, L_max] and zero-pad outside [0, L]\n",
        "        x = np.linspace(0, L_range[1], NUM_POINTS, endpoint=False)\n",
        "        psi = np.where(x <= L,\n",
        "                       np.sqrt(2 / L) * np.sin(n * np.pi * x / L),\n",
        "                       0.0)\n",
        "        X[i] = psi + np.random.normal(0, noise_std, NUM_POINTS)\n",
        "        y[i] = n - 1\n",
        "    return X, y\n"
      ],
      "metadata": {
        "id": "AKkicyipS88_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code here to\n",
        "# - put the generate_varying_L data into data loaders\n",
        "# - use those data to retrain the CNN\n",
        "# - use the history output to make the plots of loss and accuracy\n"
      ],
      "metadata": {
        "id": "gTgJ9JJ8sF3i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## part 2(e) (5 points)\n",
        "\n",
        "Now for the biggest challenge of all.\n",
        "\n",
        "Each sample is now a normalized superposition\n",
        "\n",
        "$$\\Psi(x) = \\alpha\\,\\psi_m(x) + \\beta\\,\\psi_n(x), \\qquad \\alpha^2 + \\beta^2 = 1$$\n",
        "\n",
        "where $m \\neq n$ are drawn at random and $|\\alpha| > |\\beta|$ by construction.\n",
        "The label is the **dominant** quantum number $m$.\n",
        "\n",
        "The CNN will have to figure out the dominant quantum number.\n",
        "\n",
        "This is significantly harder than the previous challenges because the CNN must disentangle overlapping spatial\n",
        "frequencies.\n",
        "\n",
        "Use the function below to generate the data; then retrain the CNN to find the dominant quantum number.\n"
      ],
      "metadata": {
        "id": "kyaYwOvefXRb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_superposition(n_samples, L=1.0, noise_std=0.05):\n",
        "    \"\"\"Generate superpositions of two states; label = dominant n.\"\"\"\n",
        "    x = np.linspace(0, L, NUM_POINTS, endpoint=False)\n",
        "    X = np.zeros((n_samples, NUM_POINTS))\n",
        "    y = np.zeros(n_samples, dtype=int)\n",
        "\n",
        "    for i in range(n_samples):\n",
        "        # Pick two distinct quantum numbers\n",
        "        m, n = np.random.choice(range(1, NUM_CLASSES + 1), size=2, replace=False)\n",
        "\n",
        "        # Random mixing angle; ensure |alpha| > |beta|\n",
        "        theta = np.random.uniform(0, np.pi / 4)     # alpha = cos θ ≥ cos(π/4) ≈ 0.71\n",
        "        alpha, beta = np.cos(theta), np.sin(theta)\n",
        "\n",
        "        psi_m = np.sqrt(2 / L) * np.sin(m * np.pi * x / L)\n",
        "        psi_n = np.sqrt(2 / L) * np.sin(n * np.pi * x / L)\n",
        "        psi   = alpha * psi_m + beta * psi_n\n",
        "\n",
        "        X[i] = psi + np.random.normal(0, noise_std, NUM_POINTS)\n",
        "        y[i] = m - 1        # dominant quantum number\n",
        "\n",
        "    return X, y\n"
      ],
      "metadata": {
        "id": "ue3rQ6DucoHc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code here to\n",
        "# - put the generate_superposition data into data loaders\n",
        "# - use those data to retrain the CNN and plot results"
      ],
      "metadata": {
        "id": "MlmlB9tXxyyr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## part 2(f) (5 points)\n",
        "\n",
        "- Use `scikit-learn.confusion_matrix` to display the confusion matrix for the 5 labeled classes.\n",
        "- Which states were most easily confused? Explain the reason for the confusion, using your knowledge of quantum mechanics."
      ],
      "metadata": {
        "id": "_C-b5RYCuHD8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 3: LSTM Training for Orbital Mechanics (35 points)\n",
        "\n",
        "The LSTM can learn a series of positions of an orbiting body (Keplerian orbit).\n",
        "\n",
        "In this exercise you will generate the orbital data and then train the `OrbitLSTM` to predict the orbit given a starting point and a few steps.\n",
        "\n"
      ],
      "metadata": {
        "id": "idjSh7VVJJ4Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Reproducibility\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Use GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "id": "Oah6Al7gQz6x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the code to generate the dataset.\n",
        "\n",
        "At each step the state is a 4-dimensional vector:\n",
        "\n",
        "$$\\mathbf{s}(t) = (x,\\; y,\\; v_x,\\; v_y)$$\n",
        "\n",
        "The gravitational equations of motion (with $\\mu = GM = 1$) are:\n",
        "\n",
        "$$\\ddot{x} = -\\frac{x}{r^3}, \\qquad \\ddot{y} = -\\frac{y}{r^3}, \\qquad r = \\sqrt{x^2 + y^2}$$\n",
        "\n",
        "We derive initial conditions at perihelion (closest approach):\n",
        "\n",
        "$$x_0 = a(1-e), \\quad y_0 = 0, \\quad v_{x,0} = 0, \\quad v_{y,0} = \\sqrt{\\frac{\\mu(1+e)}{a(1-e)}},$$\n",
        "\n",
        "where $a$ is the semi-major axis and $e$ is the eccentricity."
      ],
      "metadata": {
        "id": "l70dPOxU6udr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def kepler_initial_conditions(a: float, e: float, mu: float = 1.0) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Compute initial state vector [x, y, vx, vy] at perihelion for a\n",
        "    Kepler orbit with semi-major axis `a` and eccentricity `e`.\n",
        "    \"\"\"\n",
        "    r_peri = a * (1.0 - e)\n",
        "    v_peri = np.sqrt(mu * (1.0 + e) / (a * (1.0 - e)))\n",
        "    return np.array([r_peri, 0.0, 0.0, v_peri])\n",
        "\n",
        "def gravitational_acceleration(pos: np.ndarray, mu: float = 1.0) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Compute gravitational acceleration at position `pos`.\n",
        "    \"\"\"\n",
        "    r = np.linalg.norm(pos)\n",
        "    return -mu * pos / r**3\n",
        "\n",
        "def integrate_orbit(state0: np.ndarray, dt: float, n_steps: int,\n",
        "                    mu: float = 1.0) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Integrate a 2D Kepler orbit using the Velocity Verlet method.\n",
        "    \"\"\"\n",
        "    trajectory = np.zeros((n_steps + 1, 4))\n",
        "    trajectory[0] = state0\n",
        "\n",
        "    pos = state0[:2].copy()\n",
        "    vel = state0[2:].copy()\n",
        "    acc = gravitational_acceleration(pos, mu)\n",
        "\n",
        "    for i in range(1, n_steps + 1):\n",
        "        pos = pos + vel * dt + 0.5 * acc * dt**2\n",
        "        acc_new = gravitational_acceleration(pos, mu)\n",
        "        vel = vel + 0.5 * (acc + acc_new) * dt\n",
        "        acc = acc_new\n",
        "        trajectory[i] = np.concatenate([pos, vel])\n",
        "\n",
        "    return trajectory\n",
        "\n",
        "def generate_orbit_dataset(n_orbits: int = 500, steps_per_orbit: int = 300,\n",
        "                            a_range: tuple = (0.8, 1.2),\n",
        "                            e_range: tuple = (0.0, 0.2),\n",
        "                            mu: float = 1.0) -> list:\n",
        "    \"\"\"\n",
        "    Generate a list of orbital trajectories with random parameters.\n",
        "    \"\"\"\n",
        "    orbits = []\n",
        "    for _ in range(n_orbits):\n",
        "        a = np.random.uniform(*a_range)\n",
        "        e = np.random.uniform(*e_range)\n",
        "        state0 = kepler_initial_conditions(a, e, mu)\n",
        "        T_orbital = 2.0 * np.pi * a**1.5\n",
        "        dt = T_orbital / steps_per_orbit\n",
        "        traj = integrate_orbit(state0, dt, steps_per_orbit, mu)\n",
        "        orbits.append(traj)\n",
        "    return orbits\n",
        "\n",
        "class OrbitDataset(Dataset):\n",
        "    \"\"\"\n",
        "    PyTorch Dataset for next-step prediction on orbital trajectories.\n",
        "\n",
        "    Each sample is:\n",
        "        X: states[i : i + seq_len]          shape (seq_len, 4)\n",
        "        Y: states[i + seq_len]              shape (4,)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, orbits: list, seq_len: int = 20):\n",
        "        self.samples_X = []\n",
        "        self.samples_Y = []\n",
        "        for traj in orbits:\n",
        "            for i in range(len(traj) - seq_len):\n",
        "                self.samples_X.append(traj[i:i + seq_len])\n",
        "                self.samples_Y.append(traj[i + seq_len])\n",
        "        self.samples_X = np.array(self.samples_X, dtype=np.float32)\n",
        "        self.samples_Y = np.array(self.samples_Y, dtype=np.float32)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples_X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return (torch.tensor(self.samples_X[idx]),\n",
        "                torch.tensor(self.samples_Y[idx]))"
      ],
      "metadata": {
        "id": "1CmEMdmFz2Q-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate the data\n",
        "print(\"Generating training orbits...\")\n",
        "train_orbits = generate_orbit_dataset(n_orbits=400, steps_per_orbit=300)\n",
        "print(\"Generating validation orbits...\")\n",
        "val_orbits = generate_orbit_dataset(n_orbits=100, steps_per_orbit=300)\n",
        "\n",
        "SEQ_LEN = 30\n",
        "train_dataset = OrbitDataset(train_orbits, seq_len=SEQ_LEN)\n",
        "val_dataset = OrbitDataset(val_orbits, seq_len=SEQ_LEN)\n",
        "\n",
        "print(f\"Training samples:   {len(train_dataset):,}\")\n",
        "print(f\"Validation samples: {len(val_dataset):,}\")"
      ],
      "metadata": {
        "id": "QLvu7CKa0n0D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We generate many orbits with randomised parameters, then package them for PyTorch. The RNN receives a window of `seq_len` consecutive states and predicts the next state.\n",
        "\n",
        "\n",
        "The class `OrbitLSTM` defines the model to be trained.\n",
        "1. An stacked LSTM encoder processes the input sequence.\n",
        "2. A fully-connected head (last layer) maps the final hidden state to the predicted next state."
      ],
      "metadata": {
        "id": "_0aIQ2At6z5j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class OrbitLSTM(nn.Module):\n",
        "    \"\"\"\n",
        "    LSTM model for next-step orbital state prediction.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    input_size : int\n",
        "        Dimensionality of the state vector (default 4).\n",
        "    hidden_size : int\n",
        "        Number of LSTM hidden units.\n",
        "    num_layers : int\n",
        "        Number of stacked LSTM layers.\n",
        "    dropout : float\n",
        "        Dropout between LSTM layers (only used if num_layers > 1).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_size: int = 4, hidden_size: int = 64,\n",
        "                 num_layers: int = 3, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=input_size,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            dropout=dropout if num_layers > 1 else 0.0,\n",
        "        )\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(hidden_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, input_size),\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : Tensor of shape (batch, seq_len, 4)\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        Tensor of shape (batch, 4) — predicted next state.\n",
        "        \"\"\"\n",
        "        lstm_out, _ = self.lstm(x)         # (batch, seq_len, hidden)\n",
        "        last_hidden = lstm_out[:, -1, :]   # (batch, hidden)\n",
        "        return self.fc(last_hidden)        # (batch, 4)\n"
      ],
      "metadata": {
        "id": "uGx2cLbS0qiE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## part 3(a): Set up training (5 points)\n",
        "\n",
        "The model is already set up for you. Now\n",
        "- add your code to evaluate the average loss per epoch\n",
        "- add an optimizer and scheduler suitable for LSTM model"
      ],
      "metadata": {
        "id": "MWPK-cGG6-SJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch(model, dataloader, optimizer, loss_fn, device):\n",
        "    \"\"\"Train for one epoch. Returns average loss.\"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    for X_batch, Y_batch in dataloader:\n",
        "        X_batch, Y_batch = X_batch.to(device), Y_batch.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        pred = model(X_batch)\n",
        "        loss = loss_fn(pred, Y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item() * X_batch.size(0)\n",
        "    return total_loss / len(dataloader.dataset)\n",
        "\n",
        "\n",
        "def evaluate(model, dataloader, loss_fn, device):\n",
        "    \"\"\"Evaluate on a dataset. Returns average loss.\"\"\"\n",
        "    # Add your code to calculate the loss per batch and the total_loss\n",
        "    # You can look in the train_one_epoch function for help\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        # add your code here\n",
        "\n",
        "    return total_loss / len(dataloader.dataset)\n",
        "\n",
        "\n",
        "def train_model(model, train_loader, val_loader, n_epochs=30, lr=1e-3,\n",
        "                loss_fn=None, device=device):\n",
        "    \"\"\"\n",
        "    Full training loop with logging.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    dict with keys 'train_losses' and 'val_losses'.\n",
        "    \"\"\"\n",
        "    if loss_fn is None:\n",
        "        loss_fn = nn.MSELoss()\n",
        "\n",
        "    # Add your optimizer and scheduler here\n",
        "    optimizer =\n",
        "    scheduler =\n",
        "\n",
        "    history = {\"train_losses\": [], \"val_losses\": []}\n",
        "\n",
        "    for epoch in range(1, n_epochs + 1):\n",
        "        train_loss = train_one_epoch(model, train_loader, optimizer, loss_fn, device)\n",
        "        val_loss = evaluate(model, val_loader, loss_fn, device)\n",
        "        scheduler.step(val_loss)\n",
        "        history[\"train_losses\"].append(train_loss)\n",
        "        history[\"val_losses\"].append(val_loss)\n",
        "\n",
        "        if epoch % 5 == 0 or epoch == 1:\n",
        "            print(f\"Epoch {epoch:3d} | Train Loss: {train_loss:.6f} | Val Loss: {val_loss:.6f}\")\n",
        "\n",
        "    return history\n"
      ],
      "metadata": {
        "id": "auTRDH_904Vn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## part 3(b): Train LSTM (10 points)\n",
        "\n",
        "Train the LSTM with reasonable values of `num_layers`, `dropout`, and `hidden_size`.\n",
        "\n",
        "You will need to evaluate the performance using the cells below."
      ],
      "metadata": {
        "id": "k2sos90oBdVX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create data loaders\n",
        "BATCH_SIZE = 256\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
        "\n",
        "# Instantiate and train the baseline model with reasonable parameters\n",
        "model_baseline = OrbitLSTM(input_size=4, hidden_size=16, num_layers=2, dropout=0.30).to(device)\n",
        "print(f\"Model parameters: {sum(p.numel() for p in model_baseline.parameters()):,}\")\n",
        "print(\"\\nTraining baseline model (MSE loss)...\")\n",
        "history_baseline = train_model(model_baseline, train_loader, val_loader, n_epochs=30)\n"
      ],
      "metadata": {
        "id": "dyXApAOQ07Fc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## part 3(c): Evaluate training and validation loss (5 points)\n",
        "\n",
        "Add your code inside `plot_training_curves` to plot the training and validation loss as a function of training epoch."
      ],
      "metadata": {
        "id": "T1hHETzU7RgE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Implement the function plot_training curves to plot the training loss\n",
        "#  and validation loss as a function of training epoch\n",
        "\n",
        "def plot_training_curves(history, title=\"Training Curves\"):\n",
        "    \"\"\"Plot train and validation loss over epochs.\"\"\"\n",
        "    fig, ax = plt.subplots(figsize=(8, 4))\n",
        "    # Add code here\n",
        "    plt.show()\n",
        "\n",
        "plot_training_curves(history_baseline, \"Baseline LSTM — Training Curves\")"
      ],
      "metadata": {
        "id": "TIJKgGru070G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The real test of the training is to give the model a starting sequence of points and have it map out an entire orbit. We can compare how well it matches the results from the equations of motion."
      ],
      "metadata": {
        "id": "sJtrh1wn9wEA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rollout_trajectory(model, seed_states: np.ndarray, n_steps: int,\n",
        "                       device=device) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Autoregressively predict a trajectory.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model : OrbitLSTM\n",
        "        Trained model.\n",
        "    seed_states : np.ndarray of shape (seq_len, 4)\n",
        "        Initial window of true states to seed the rollout.\n",
        "    n_steps : int\n",
        "        Number of additional steps to predict.\n",
        "    device : torch.device\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    np.ndarray of shape (seq_len + n_steps, 4)\n",
        "        The seed states followed by predicted states.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    seq_len = seed_states.shape[0]\n",
        "    result = list(seed_states)\n",
        "\n",
        "    window = torch.tensor(seed_states, dtype=torch.float32).unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(n_steps):\n",
        "            pred = model(window)                # (1, 4)\n",
        "            pred_np = pred.cpu().numpy()[0]\n",
        "            result.append(pred_np)\n",
        "            # Slide window forward\n",
        "            new_step = pred.unsqueeze(1)        # (1, 1, 4)\n",
        "            window = torch.cat([window[:, 1:, :], new_step], dim=1)\n",
        "\n",
        "    return np.array(result)\n"
      ],
      "metadata": {
        "id": "lS0jstZD1H2n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## part 3(d): Update training to optimize performance (15 points)\n",
        "\n",
        "Test the model by plotting the prediction against the truth calculated by integrating the equations of motions.\n",
        "\n",
        "Are you satisfied with the results? If not, go back and adjust some of the model parameters and training parameters until you are satisfied.\n",
        "\n",
        "During this process you will need to think carefully about the LSTM:\n",
        "- What is it learning?\n",
        "- What are its limitations?\n",
        "- What do the training vs. validation loss tell you?\n",
        "- Are the test data in the plot well learned by the LSTM?"
      ],
      "metadata": {
        "id": "comttNYT-BOL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_rollout(model, test_orbit: np.ndarray, seq_len: int = 20,\n",
        "                     label: str = \"Model\"):\n",
        "    \"\"\"\n",
        "    Roll out a prediction and plot trajectory\n",
        "    compared to the ground truth.\n",
        "    \"\"\"\n",
        "    n_predict = len(test_orbit) - seq_len\n",
        "    seed = test_orbit[:seq_len]\n",
        "    pred_traj = rollout_trajectory(model, seed, n_predict)\n",
        "    fig, ax = plt.subplots(1, 1, figsize=(16, 5))\n",
        "\n",
        "    # Trajectory\n",
        "    ax.plot(test_orbit[:, 0], test_orbit[:, 1], 'b-', alpha=0.7, label='Ground Truth')\n",
        "    ax.plot(pred_traj[:, 0], pred_traj[:, 1], 'r--', alpha=0.7, label=label)\n",
        "    ax.plot(0, 0, 'ko', markersize=8)  # Central body\n",
        "    ax.set_xlabel('x')\n",
        "    ax.set_ylabel('y')\n",
        "    ax.set_title('Orbital Trajectory')\n",
        "    ax.legend()\n",
        "    ax.set_aspect('equal')\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.suptitle(f\"{label} — Autoregressive Rollout\", fontsize=14, y=1.02)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Generate a test orbit (moderate eccentricity)\n",
        "a_test, e_test = 1.0, 0.3\n",
        "state0_test = kepler_initial_conditions(a_test, e_test)\n",
        "T_test = 2 * np.pi * a_test**1.5\n",
        "test_orbit = integrate_orbit(state0_test, T_test / 300, 300)\n",
        "\n",
        "baseline_metrics = evaluate_rollout(model_baseline, test_orbit, seq_len=SEQ_LEN,\n",
        "                                     label=\"Baseline LSTM\")\n"
      ],
      "metadata": {
        "id": "84wSAeX_1PcC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hazVh5UO2rpa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}