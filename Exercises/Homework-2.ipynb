{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RCW83RcF1YIx"
      },
      "source": [
        "# Exercise 1: Decision Tree Classifier (30 points)\n",
        "\n",
        "In this exercise we will train a decision tree classifier for neutrino experiment data from [the MiniBOONE experiment](https://www.fnal.gov/pub/science/experiments/intensity/miniboone.html).\n",
        "\n",
        "The goal is to prepare a binary classification to distinguish electron neutrinos (signal) from muon neutrinos (background). The dataset is taken from the UCI ML dataset repository."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U-C1caVlv32H"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "data = fetch_openml(\"miniboone\", parser=\"auto\", version=1)\n",
        "X, y = data[\"data\"].values, (data[\"target\"].values == \"True\").astype(float)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfEdPdk663UZ"
      },
      "source": [
        "## part 1(a): Data Exploration and Visualization (10 points)\n",
        "\n",
        "Explore the dataset before building your ML classifier:\n",
        "- How many events are in the training data and testing data?\n",
        "- How many input features are there for each point in the training data?\n",
        "- Plot at least three of the input features for events with `y_train==0` and for events with `y_train==1`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mbXFqZnwDMU5"
      },
      "outputs": [],
      "source": [
        "# Add code here to explore the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gMmzYnAD67cd"
      },
      "source": [
        "## part 1(b): Decision Tree (10 points)\n",
        "\n",
        "Train a simple `DecisionTreeClassifier` with `max_depth=5` and the `gini` criterion to separate the electron neutrino signal and muon neutrino background.\n",
        "\n",
        "Calculate the performance of the Decision Tree:\n",
        "- confusion matrix\n",
        "- ROC curve\n",
        "- Area Under Curve (AUC)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qBNI89UdDLeO"
      },
      "outputs": [],
      "source": [
        "# Add code here to train the DecisionTreeClassifier and assess its performance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0BQiJcdX69wn"
      },
      "source": [
        "## part 1(c): Gini impurity measure\n",
        "\n",
        "Visualize the decision tree, and pick three of the leaf nodes in your decision tree visualization.\n",
        "\n",
        "For each node, calculate the Gini impurity by hand to check against the calculation from scikit-learn. Show the steps of your calculation and the result."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LGW-1iAX6217"
      },
      "outputs": [],
      "source": [
        "# Add code to visualize the decision tree and get impurity measures"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6oN6EQN41mhi"
      },
      "source": [
        "# Exercise 2: Backpropagation (30 points)\n",
        "\n",
        "In this exercise you will gain some experience with the backpropagation equations and the numerical calculations of the gradient.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7BSKhE-Or9cM"
      },
      "source": [
        "## part 2(a): Backpropagation theory (15 points)\n",
        "\n",
        "Complete the proofs of [Nielsen's equations BP3 and BP4](http://neuralnetworksanddeeplearning.com/chap2.html#proof_of_the_four_fundamental_equations_(optional)).\n",
        "\n",
        "In Nielsen's notation (but with layer numbers written in parenthesis for clarity):\n",
        "$$\n",
        "\\begin{align}\n",
        "\\frac{\\partial C}{\\partial b_j^{(l)}} &= \\delta_j^{(l)} & \\quad \\quad \\text{(BP3)} \\\\\n",
        "\\frac{\\partial C}{\\partial w_{jk}^{(l)}} &= a_k^{(l-1)} \\delta_j^{(l)} & \\quad \\quad \\text{(BP4)}\n",
        "\\end{align}\n",
        "$$\n",
        "where $C$ is the cost function (loss function), $w$ and $b$ are weights and biases, $a$ is the post-activation output for the layer, and $\\delta$ is the error of a single neuron."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WopNgzR_1-wW"
      },
      "source": [
        "## part 2(b): Numerical calculation (15 points)\n",
        "\n",
        "Consider a single perceptron node with 1-dimensional input $x$. The weight and bias of the node are $w=0.5$ and $b=0.1$. We use the ReLU activation function on the output of the node and the mean square error loss function.\n",
        "\n",
        "If the data input and target are $x=1, y=1$:\n",
        "- What is the loss function value?\n",
        "- What is the gradient $\\partial L/\\partial w$?\n",
        "- What is the gradient $\\partial L/\\partial b$?\n",
        "\n",
        "You can do this by hand. Show your work, and feel free to check your answer with some code.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ArVWPrXb1rZ0"
      },
      "source": [
        "# Exercise 3: Deep Neural Networks (40 points)\n",
        "\n",
        "In this exercise we will train a deep neural network for neutrino experiment data from [the MiniBOONE experiment](https://www.fnal.gov/pub/science/experiments/intensity/miniboone.html).\n",
        "\n",
        "The goal is similar to Exercise 1 above: prepare a binary classification to distinguish electron neutrinos (signal) from muon neutrinos (background).\n",
        "\n",
        "In this exercise, you will set up a neural network model using PyTorch and train it using the MiniBOONE training data, then test it on the testing data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EMOwMfwg-L53"
      },
      "source": [
        "## part 3(a) (20 points): DNN with tanh activation\n",
        "\n",
        "Implement a deep neural network (at least 1 hidden layer between the input and output layers) to classify events as electron neutrinos (signal) or muon neutrinos (background). Use the tanh activation function, except in the output layer.\n",
        "\n",
        "Here are some starting points for the DNN:\n",
        "- 3 hidden layers w/ 64 units each\n",
        "- BCE loss function\n",
        "- SGD optimizer w/ batch size of 128\n",
        "\n",
        "You should expect to train for at least 50 epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DhhTXotYAcP4"
      },
      "outputs": [],
      "source": [
        "# Put code here to implement the DNN to classify neutrino events\n",
        "# You can use the examples from class and the hands-on notebooks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iGtDW-L0-Q-2"
      },
      "source": [
        "## part 3(b) (10 points): DNN performance assessment\n",
        "\n",
        "Calculate the performance of the DNN on the testing data:\n",
        "- confusion matrix\n",
        "- ROC curve\n",
        "- Area Under Curve (AUC)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6F5W0mNEBrnA"
      },
      "outputs": [],
      "source": [
        "# Put code here to assess the DNN performance\n",
        "# You can use the examples from class and the hands-on notebooks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LtdnSpN5-Oeq"
      },
      "source": [
        "## part 3(c) (10 points): Change of activation function\n",
        "\n",
        "Replace the tanh activation functions with the ReLU activation functions, in the layers where it is possible.\n",
        "(Hint: there is one layer where you can't use the ReLU activation function--why not?)\n",
        "\n",
        "What happens with the training and performance? Can you improve the performance relative to the original training with sigmoid activation functions?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4HSDBLP010Dc"
      },
      "outputs": [],
      "source": [
        "# Put code here to change the activation functions to ReLU,\n",
        "# then retrain and re-assess the performance.\n",
        "# (It's reasonable to cut-and-paste from the code in part 3(a).)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l-7vCXqzCsK5"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
