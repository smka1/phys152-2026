{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Ising Model exploration\n",
        "\n",
        "The actual Ising model Hamiltonian is\n",
        "$$H_\\mathrm{model}[\\boldsymbol{S}^i] = - \\sum_{j=1}^L \\sum_{k=1}^L J_{j,k}S_{j}^iS_{k}^i = - \\sum_{j=1}^L \\sum_{k=1}^L S_{j}^iS_{k}^i J_{j,k}.$$\n",
        "\n",
        "Following Mehta et al., we recast this model in the form\n",
        "$$\n",
        "H_\\mathrm{model}^i \\equiv \\mathbf{X}^i \\cdot \\mathbf{J},\n",
        "$$\n",
        "\n",
        "where the vectors $\\mathbf{X}^i$ represent all two-body interactions $\\{S_{j}^iS_{k}^i \\}$ with indices $j,k$ ranging from 1 to $L$ (the size of the array), and the index $i$ runs over the input configuration states in the data set.\n",
        "\n",
        "Our goal will be to learn the interaction strength ${\\bf J}$.\n",
        "\n",
        "## Prepare training data\n",
        "\n",
        "First we create the datasets. We do this by using our knowledge of the Ising model to calculate the correct energy of each state. If we were doing this experimentally, we would measure the energy of each configuration instead of simply calculating it.\n",
        "\n",
        "Actually, we're going to start with a really small example dataset and return later to a larger physical dataset."
      ],
      "metadata": {
        "id": "fztqO6M03PDL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "np.random.seed(100)\n",
        "\n",
        "### Ising model size\n",
        "L=10\n",
        "\n",
        "# create 1 random 1-d Ising state configurations\n",
        "states=np.random.choice([-1, 1], size=(1,L))\n",
        "\n",
        "def ising_energies(states):\n",
        "    \"\"\"\n",
        "    This function calculates the energies of the states in the nn Ising Hamiltonian\n",
        "    \"\"\"\n",
        "    L = states.shape[1]\n",
        "    J = np.zeros((L, L),)\n",
        "    for i in range(L):\n",
        "        J[i,(i+1)%L]=-1.0 # interaction between nearest-neighbors\n",
        "\n",
        "    # compute energies\n",
        "    E = np.einsum('...i,ij,...j->...',states,J,states)\n",
        "\n",
        "    return E\n",
        "# calculate Ising energies\n",
        "energies=ising_energies(states)"
      ],
      "metadata": {
        "id": "g6sOc0IZFwa0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is always a good idea to check the dataset yourself.\n",
        "You will get some idea of the range of data values, the shape of the input vector, and the Python data type."
      ],
      "metadata": {
        "id": "MTsXPBH1WZNs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(states), \"states =\", states)\n",
        "print(len(energies), \"energies = \", energies)"
      ],
      "metadata": {
        "id": "sNXiXjz9WXNk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next part is a bit tricky. We are trying to compute all of the interactions between different spin locations, not just the nearest neighbors. In other words, we're trying to write out $S_j S_k$ for all $j,k$ pairs. This means we will end up with a matrix of size $L \\times L$.\n",
        "\n",
        "Formally, this is the same as computing the outer product of the `states` vector with itself.\n",
        "If you write the states vector as ${\\bf S}$, then this is constructing ${\\bf S}{\\bf S}^T \\equiv {\\bf X}$.\n",
        "There are several ways to do this: the most transparent way is to iterate over $j$ and $k$, and the most compact way is to use the NumPy `einsum` format.\n",
        "\n",
        "Then we flatten the $L \\times L$ array of interactions into a $1 \\times L^2$ array called `flattened_states`.\n",
        "As we have seen in the past, it is straightforward to create a Torch tensor from an array like that."
      ],
      "metadata": {
        "id": "-Qd9mBATXm5h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# reshape Ising states into LR samples: S_i S_j --> X_p\n",
        "# effectively it is the outer product of the states\n",
        "op_states=np.einsum('...i,...j->...ij', states, states)\n",
        "print(op_states)\n",
        "shape=op_states.shape\n",
        "print(shape)\n",
        "flattened_states=op_states.reshape((shape[0],shape[1]*shape[2]))\n",
        "# build final data set\n",
        "Data=[flattened_states,energies]\n",
        "print(Data)"
      ],
      "metadata": {
        "id": "bO2ZNNnK3ig4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "input_features = torch.from_numpy(np.stack([state for state in Data[0]])).float()\n",
        "print(input_features)\n",
        "output = torch.tensor([[e] for e in Data[1]], dtype=torch.float32)\n",
        "print(output)"
      ],
      "metadata": {
        "id": "v76-upH6S6qo",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are creating a simple linear model in which the weights are the same as the interaction strength.\n",
        "\n",
        "You can see from the model that if the input features are the spin-spin correlations ${\\bf X}$, then the interaction strength ${\\bf J}$ is the same as the weights for linear model.\n",
        "\n",
        "We should not use the sigmoid activation function -- why not? (Think about the target output values.)"
      ],
      "metadata": {
        "id": "wf5sHts-fXPg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn, optim\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(L*L, 1)\n",
        ")\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1) # Reduced learning rate"
      ],
      "metadata": {
        "id": "L_nb44EN4LOp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the training loop\n",
        "for epoch in range(10):\n",
        "\n",
        "  # Forward pass: Calculate the predicted output class using the model\n",
        "  predicted_energies = model(input_features)\n",
        "\n",
        "  # Calculate the loss between the predicted output class and the actual target class\n",
        "  loss = criterion(predicted_energies, output)\n",
        "\n",
        "  # Backpropagation: Compute gradients of the model parameters with respect to the loss\n",
        "  loss.backward()\n",
        "  # Update the model parameters using the computed gradients\n",
        "  optimizer.step()\n",
        "  # Zero out the gradients for the next iteration to avoid accumulation\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  if epoch % 1000 == 0:\n",
        "      print(f\"Epoch {epoch} done! Loss = {loss}\")"
      ],
      "metadata": {
        "id": "e77YpNcFQKH-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "for name, param in model.named_parameters():\n",
        "    if name == '0.weight':\n",
        "        weight_tensor = param.data\n",
        "array_2d = weight_tensor.reshape(L, L)\n",
        "\n",
        "# Plot\n",
        "plt.imshow(array_2d, cmap='viridis')\n",
        "plt.colorbar()\n",
        "plt.title('2-D Array of $J_{jk}$ interaction strengths')\n",
        "plt.xlabel('j')\n",
        "plt.ylabel('k')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FPBsyDkgh5EQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here are things for you to explore:\n",
        "- Does changing the learning rate help or hurt the convergence? What is the maximum useful learning rate? Does it depend on the number of spin states?\n",
        "- What if the interaction strength were doubled in the Ising model? Create a new dataset with that change and see if your model can learn the new interaction strength.\n",
        "- Could you use the cross-entropy loss function here? Why or why not?"
      ],
      "metadata": {
        "id": "S6k3bMwvaWFO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Collider dataset example\n",
        "\n",
        "One of the challenges for machine learning generally, and deep learning specifically, is obtaining a large labeled training dataset.\n",
        "These large datasets are the key to fully fitting a complex deep learning model to data.\n",
        "If there are not enough data points then the model will be *overfit* to the small training data sample.\n",
        "\n",
        "Collider physics experiments generate huge particle physics datasets because of the high collision rate.\n",
        "Each input data point vector $\\vec{v}^i$ represents a single particle collision.\n",
        "The components of the data point $\\vec{v}^i_j$ represent features of the specific collision, like the momenta of the outgoing particles.\n",
        "\n",
        "## Search for supersymmetry\n",
        "\n",
        "We will work with the [SUSY dataset](https://archive.ics.uci.edu/ml/machine-learning-databases/00279/SUSY.csv.gz), available from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets.html)\n",
        "This is a comprehensive repository of useful datasets relevant to ML.\n",
        "The SUSY dataset was prepared by UC Irvine colleagues from the ATLAS experiment.\n",
        "\n",
        "Here is the description of the SUSY dataset we will be playing around with for this notebook:\n",
        ">The data has been produced using Monte Carlo simulations and contains events with two leptons (electrons or muons). In high energy physics experiments, such as the ATLAS and CMS detectors at the CERN LHC, one major hope is the discovery of new particles. To accomplish this task, physicists attempt to sift through data events and classify them as either a signal of some new physics process or particle, or instead a background event from understood Standard Model processes. Unfortunately we will never know for sure what underlying physical process happened (the only information to which we have access are the final state particles). However, we can attempt to define parts of phase space that will have a high percentage of signal events. Typically this is done by using a series of simple requirements on the kinematic quantities of the final state particles, for example having one or more leptons with large amounts of momentum that is transverse to the beam line ($p_{T}$).\n",
        "\n",
        "Here instead we will use logistic regression in order to attempt to find out the relative probability that an event is from a signal or a background event and rather than using the kinematic quantities of final state particles directly we will use the output of our logistic regression to define a part of phase space that is enriched in signal events.\n",
        "\n",
        "The dataset we are using has values of 18 kinematic variables (\"features\") for each the event. The first 8 features are direct measurements of final state particles, in this case the $p_{T}$, pseudo-rapidity ($\\eta$), and azimuthal angle ($\\phi$) of two leptons in the event and the amount of missing transverse momentum (MET) together with its azimuthal angle. The last ten features are functions of the first 8 features; these are high-level features derived by physicists to help discriminate between the two classes. You can think of them as physicists attempt to use non-linear functions to classify signal and background events and they have been developed with a lot of deep thinking on the part of physicist.\n",
        "\n",
        "The dataset consists of 5 million events We will use the first 1,000,000 for training the model.\n",
        "\n",
        "## Classification task\n",
        "\n",
        "The training data consists of a set of features and discrete labels. This type of data is called categorical data (the data comes in different categories).\n",
        "\n",
        "In the SUSY dataset, the goal is to decide whether a data point represents signal \"potential collision\"- labeled 1, or \"background\"(Standard Model processes which produce final states with similar constituents as SUSY processes) - labeled 0. This is done by looking at the 18 input features - the first 8 of which are \"low-level\" features that can be directly measured and the last 10 features are \"higher-order\" features constructed using physics intuition. In more detail:\n",
        ">The first column is the class label (1 for signal, 0 for background), followed by the 18 features (8 low-level features then 10 high-level features):: lepton 1 pT, lepton 1 eta, lepton 1 phi, lepton 2 pT, lepton 2 eta, lepton 2 phi, missing energy magnitude, missing energy phi, MET_rel, axial MET, M_R, M_TR_2, R, MT2, S_R, M_Delta_R, dPhi_r_b, cos(theta_r1)\n",
        "\n",
        "Our goal will be to use either the first 8 features or the full 18 features to predict whether an event is signal or background.\n",
        "\n",
        "The cross-entropy loss function is the right choice for this kind of classification task.\n",
        "\n"
      ],
      "metadata": {
        "id": "vP6riJVhsS0e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing the data set\n",
        "\n",
        "The dataset is large (how large?), so we will use a custom class to define the PyTorch tensors.\n",
        "The advantage is that we don't need to download the file, convert it in memory, and then copy the data to a tensor.\n",
        "All of that happens in one pass inside `CSVDataset`.\n",
        "We added a parameter to limit the size of the dataset while we are debugging; otherwise you will get the full 5 million events!\n",
        "\n",
        "The AI agents are very very good at helping you figure out how to import data and reshape it as needed for training."
      ],
      "metadata": {
        "id": "rw0gsvIzkKjp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class CSVDataset(Dataset):\n",
        "    def __init__(self, csv_file, max_samples=None):\n",
        "        self.data = pd.read_csv(csv_file)\n",
        "        if max_samples:\n",
        "            self.data = self.data.iloc[:max_samples]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.data.iloc[idx]\n",
        "        features = torch.tensor(row.iloc[1:].values, dtype=torch.float32)\n",
        "        label = torch.tensor(row.iloc[0], dtype=torch.float32)\n",
        "        return features, label\n",
        "\n",
        "# Use it with DataLoader\n",
        "dataset = CSVDataset('https://archive.ics.uci.edu/ml/machine-learning-databases/00279/SUSY.csv.gz', 10000)\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)"
      ],
      "metadata": {
        "id": "9bKWBBhforfo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "How would you change the CSVDataset function to load just the first 8 input features instead of all of them?\n",
        "\n",
        "It is good to check the data format for one input vector before creating the model.\n",
        "Here we look at the input features and sample label for the first record (first collision event) in the dataset."
      ],
      "metadata": {
        "id": "7m9vR8zRa6Dh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(dataset))\n",
        "\n",
        "# Get a single batch\n",
        "batch_features, batch_labels = next(iter(dataloader))\n",
        "\n",
        "print(f\"Features shape: {batch_features.shape}\")\n",
        "print(f\"Labels shape: {batch_labels.shape}\")\n",
        "print(f\"\\nFirst sample features:\\n{batch_features[0]}\")\n",
        "print(f\"\\nFirst sample label: {batch_labels[0]}\")"
      ],
      "metadata": {
        "id": "R3PskPNQmCQB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we are ready to create a neural network model to learn the relationship between the input features and the sample label.\n",
        "\n",
        "- What has to change in the code if we change the number of input features?\n",
        "- Why is there only one output in this model?\n",
        "- What is a reasonable number of nodes in the hidden layer?\n",
        "- How would you add hidden layers to the model?\n",
        "- What loss function is chosen here, and why?"
      ],
      "metadata": {
        "id": "llkZcY5Xl5_3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(18, 30),\n",
        "    nn.Sigmoid(),\n",
        "    nn.Linear(30, 1),\n",
        "    nn.Sigmoid()\n",
        ")\n",
        "\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "num_epochs = 500\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    for batch_features, batch_labels in dataloader:\n",
        "        # Forward pass\n",
        "        predictions = model(batch_features)\n",
        "        # Reshape batch_labels to match predictions\n",
        "        loss = criterion(predictions, batch_labels.unsqueeze(1))\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
      ],
      "metadata": {
        "collapsed": true,
        "id": "uxL1XvLvlz6V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Make sure you understand each line in the training loop over epochs. If something is not clear, try asking your AI agent to explain the line to you.\n",
        "\n",
        "- Does the loss decrease smoothly with the number of training epochs?\n",
        "- What explains the behavior?\n",
        "\n",
        "If it is not smooth, try some ideas to make it more smooth and decrease the overall loss value at the end of the training.\n",
        "\n",
        "Now we will look at the results of the classification by comparing the NN prediction with the correct label. What has the neural network learned?"
      ],
      "metadata": {
        "id": "WhJRH0vLb0e4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# After training, evaluate on a batch\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    sample_features, sample_labels = next(iter(dataloader))\n",
        "    outputs = model(sample_features)\n",
        "    predictions = (outputs > 0.5).float()\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.scatter(range(len(sample_labels)), sample_labels.numpy(),\n",
        "            label='True Labels', alpha=0.6, s=100)\n",
        "plt.scatter(range(len(predictions)), predictions.numpy(),\n",
        "            label='Predictions', alpha=0.6, marker='x', s=100)\n",
        "plt.xlabel('Sample Index')\n",
        "plt.ylabel('Class (0 or 1)')\n",
        "plt.legend()\n",
        "plt.title('Predictions vs True Labels')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BYcNYppDmmBl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we will look at a summary plot of the training results.\n",
        "The plot below is called the *confusion matrix*.\n",
        "Which cells represent confusion on the part of the algorithm?"
      ],
      "metadata": {
        "id": "d3bqH0ryejHX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "# Get all predictions\n",
        "model.eval()\n",
        "all_predictions = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch_features, batch_labels in dataloader:\n",
        "        outputs = model(batch_features)\n",
        "        predictions = (outputs > 0.5).float()\n",
        "        all_predictions.extend(predictions.numpy())\n",
        "        all_labels.extend(batch_labels.numpy())\n",
        "\n",
        "# Create confusion matrix\n",
        "cm = confusion_matrix(all_labels, all_predictions)\n",
        "\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.ylabel('True Label')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "22HdDlnfoe2t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It would be nice to minimize the number of events in the confusion cells, where the NN has misclassified the collision event.\n",
        "\n",
        "Do you have some ideas on how to improve the model?"
      ],
      "metadata": {
        "id": "DPZ-aB0Le4yn"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uOAod_7rfB1k"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}