{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Back-Propagation\n",
        "\n",
        "Our main purpose in this session is to demonstrate how PyTorch can calculate the gradient of a general model function and then a neural network model.\n",
        "\n",
        "## Simple function\n",
        "\n",
        "Let's define a simple 1-dimensional algebraic function, just to simplify things: $y(x) = x^2 + 3x$.\n",
        "\n",
        "We will operate on a PyTorch tensor `x`, keeping track of the computational graph along the way, then compute the gradients in the backward pass. (Remember the gradients are calculated from the output layer $l=n$ to preceding layers $n-1, n-2, \\ldots, 1$."
      ],
      "metadata": {
        "id": "JOxAMrv12isU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SH2WwDhOpGjt",
        "outputId": "d68189b9-0b5a-49d1-e5f0-bfb4956a46a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([7., 9.])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# 1. Define tensor with requires_grad=True\n",
        "x = torch.tensor([2.0, 3.0], requires_grad=True)\n",
        "\n",
        "# 2. Perform operations (Computational Graph created here)\n",
        "y = x**2 + 3*x\n",
        "loss = y.sum()\n",
        "\n",
        "# 3. Compute gradients (Backward Pass)\n",
        "loss.backward()\n",
        "\n",
        "# 4. Access gradients\n",
        "print(x.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is an example of a simple $y=f(x)$. Even though we define ${\\bf x}=(2,3)$, we are really using only one $x$-value at a time. The tensor is actually a set of 2 data points, each with a single input feature.\n",
        "\n",
        "You can see that we actually get the calculated gradients for each of the input data points.\n",
        "\n",
        "Be aware that you should only call `loss.backward` once because the `grad` function gradients accumulate with each call.\n",
        "\n",
        "This is why we call\n",
        "`optimizer.zero_grad()` or `model.zero_grad()` or `tensor.grad.zero_()` after each training batch to prevent them from interfering with the next batch's calculations.\n",
        "\n",
        "What happens if you call `loss.backward()` too many times?"
      ],
      "metadata": {
        "id": "a2kp5AdME0pI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss.backward()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "j-dyrqpvEfNy",
        "outputId": "5f4502eb-9809-4e12-ab2f-73129b80a50b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2859123600.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             )\n\u001b[0;32m--> 625\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    626\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    355\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    839\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 841\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    842\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Neural Network\n",
        "\n",
        "Now we'll calculate gradients for a deep neural network (multi-layer perceptron) with a single hidden layer.\n",
        "\n",
        "For this exercise we will use the ReLU activation function instead of the sigmoid function."
      ],
      "metadata": {
        "id": "gYHKTDBQD703"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "torch.manual_seed(42)\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(2, 3),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(3, 1),\n",
        "    nn.ReLU()\n",
        ")"
      ],
      "metadata": {
        "id": "YJw_Fsepxcu2"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "While the computational graph might be set up in a symbolic way, the actual gradients require some values in the model. (Why?)\n",
        "\n",
        "If we don't have any output targets, then we cannot calculate a loss function value, so we cannot calculate a gradient for the weights or biases.\n",
        "\n",
        "Let's set some random data, just so we have something to work with. Note the size of the input features and the output targets."
      ],
      "metadata": {
        "id": "skcNJaYgEMXN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a small batch of data compatible with our model\n",
        "batch_size = 3\n",
        "input_features = 2\n",
        "num_classes = 1\n",
        "\n",
        "# Random input data\n",
        "X = torch.randn(batch_size, input_features)\n",
        "# Random target labels\n",
        "y = torch.tensor([0, 1, 0], dtype=torch.float32).unsqueeze(1)"
      ],
      "metadata": {
        "id": "bNdGzMCQx6dM"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can feed our input data forward through the DNN model.\n",
        "\n",
        "We define a loss function so that we can get a gradient $\\partial L/\\partial w$.\n",
        "Then we clear the gradient data structure and perform the backpropagation through the network model."
      ],
      "metadata": {
        "id": "9Wje-jmdElCG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output = model(X)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "loss = criterion(output, y)\n",
        "\n",
        "# Zero out any existing gradients\n",
        "model.zero_grad()\n",
        "\n",
        "# Perform backpropagation\n",
        "loss.backward()"
      ],
      "metadata": {
        "id": "CAh5IDngGO_e"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can peek into the `grad` object, stored as part of the model, to see the numerical values for the gradient.\n",
        "\n",
        "Do you think the numerical values will change when the input dataset changes? Will this be a problem?"
      ],
      "metadata": {
        "id": "JUQDn1ylE8MN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for name, param in model.named_parameters():\n",
        "    if param.grad is not None:\n",
        "        print(f\"\\n{name}:\")\n",
        "        print(f\"  Shape: {param.grad.shape}\")\n",
        "        print(f\"  Mean gradient: {param.grad.mean().item():.6f}\")\n",
        "        print(f\"  Std gradient: {param.grad.std().item():.6f}\")\n",
        "        print(f\"  Gradient (first few values):\\n  {param.grad.flatten()[:5]}\")\n",
        "    else:\n",
        "        print(f\"\\n{name}: No gradient computed\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S9k-shgayhfp",
        "outputId": "f3016ebe-f685-4a6f-b969-66cee465d6b7"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "0.weight:\n",
            "  Shape: torch.Size([3, 2])\n",
            "  Mean gradient: 0.000342\n",
            "  Std gradient: 0.057776\n",
            "  Gradient (first few values):\n",
            "  tensor([-0.0002, -0.0829,  0.0002,  0.0982, -0.0127])\n",
            "\n",
            "0.bias:\n",
            "  Shape: torch.Size([3])\n",
            "  Mean gradient: 0.013294\n",
            "  Std gradient: 0.046669\n",
            "  Gradient (first few values):\n",
            "  tensor([-0.0399,  0.0473,  0.0325])\n",
            "\n",
            "2.weight:\n",
            "  Shape: torch.Size([1, 3])\n",
            "  Mean gradient: 0.151282\n",
            "  Std gradient: 0.062465\n",
            "  Gradient (first few values):\n",
            "  tensor([0.0827, 0.1662, 0.2049])\n",
            "\n",
            "2.bias:\n",
            "  Shape: torch.Size([1])\n",
            "  Mean gradient: 0.300815\n",
            "  Std gradient: nan\n",
            "  Gradient (first few values):\n",
            "  tensor([0.3008])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1126098768.py:6: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at /pytorch/aten/src/ATen/native/ReduceOps.cpp:1857.)\n",
            "  print(f\"  Std gradient: {param.grad.std().item():.6f}\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Can you think of any reason we care about the mean of the gradient or the standard deviation of the gradient?\n",
        "\n",
        "Do you expect the gradients of the weights and biases to be similar or different?"
      ],
      "metadata": {
        "id": "ahjhQsiKFNN_"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "W4Aqh7SgyU-G"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}