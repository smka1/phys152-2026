{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Neural Network structure\n",
        "\n",
        "How should we think about the \"layers\" and the \"nodes?\"\n",
        "When we say \"1 hidden layer,\" what does that mean?\n",
        "\n",
        "Let's build a network model and count the weights and biases we have there."
      ],
      "metadata": {
        "id": "hNhuDaQkETLl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z_eB4GfyBvqA"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(2, 3),\n",
        "    nn.Sigmoid(),\n",
        "    nn.Linear(3, 1),\n",
        "    nn.Sigmoid()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "id": "WRhKRPUdHW9_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for name, param in model.named_parameters():\n",
        "    if name == '0.weight':\n",
        "        weight_tensor = param.data\n",
        "    if name == '0.bias':\n",
        "        bias_tensor = param.data\n",
        "\n",
        "print(weight_tensor)\n",
        "print(bias_tensor)"
      ],
      "metadata": {
        "id": "NjCJpKEUHZtl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchview\n",
        "from torchview import draw_graph\n",
        "import torchvision.models as models\n",
        "\n",
        "# Draw the graph\n",
        "model_graph = draw_graph(model, input_size=(1, 2), expand_nested=False)\n",
        "model_graph.visual_graph"
      ],
      "metadata": {
        "id": "FRuKu0PCHlpP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SUSY classification with Neural Networks and Decision Trees\n",
        "\n",
        "If decision trees are great with structured data, then we expect them to do very well with the SUSY ML dataset.\n",
        "\n",
        "Let's compare the performance for the simple DecisionTreeClassifier, XGBoost, and a deep neural network.\n",
        "\n",
        "First we should develop some standards for the training set and the testing set.\n",
        "What is a reasonable split of the data between training and testing?"
      ],
      "metadata": {
        "id": "su1Xf3gPLhbn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "\n",
        "class CSVDataset(Dataset):\n",
        "    def __init__(self, csv_file, max_samples=None):\n",
        "        self.data = pd.read_csv(csv_file)\n",
        "        if max_samples:\n",
        "            self.data = self.data.iloc[:max_samples]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.data.iloc[idx]\n",
        "        features = torch.tensor(row.iloc[1:].values, dtype=torch.float32)\n",
        "        label = torch.tensor(row.iloc[0], dtype=torch.float32)\n",
        "        return features, label"
      ],
      "metadata": {
        "id": "exc8gVrIMRnW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use it with DataLoader\n",
        "dataset = CSVDataset('https://archive.ics.uci.edu/ml/machine-learning-databases/00279/SUSY.csv.gz', 10000)\n",
        "\n",
        "# Define split ratios (80% train, 20% test)\n",
        "train_size = int(0.8 * len(dataset))\n",
        "test_size = len(dataset) - train_size\n",
        "\n",
        "# Split the dataset\n",
        "train_dataset, test_dataset = random_split(\n",
        "    dataset,\n",
        "    [train_size, test_size],\n",
        "    generator=torch.Generator().manual_seed(42)  # For reproducibility\n",
        ")\n",
        "\n",
        "print(f\"Training samples: {len(train_dataset)}\")\n",
        "print(f\"Testing samples: {len(test_dataset)}\\n\")\n"
      ],
      "metadata": {
        "id": "k0KDKnAQPO2X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DataLoaders handle batching, shuffling, and parallel loading\n",
        "training_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "testing_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=True)"
      ],
      "metadata": {
        "id": "Vgdkk0Eodktu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will make sure the training is done on the training dataset."
      ],
      "metadata": {
        "id": "N-h90reolyRE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(18, 30),\n",
        "    nn.Sigmoid(),\n",
        "    nn.Linear(30, 1),\n",
        "    nn.Sigmoid()\n",
        ")\n",
        "\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "num_epochs = 50\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    for batch_features, batch_labels in training_dataloader:\n",
        "        # Forward pass\n",
        "        predictions = model(batch_features)\n",
        "        # Reshape batch_labels to match predictions\n",
        "        loss = criterion(predictions, batch_labels.unsqueeze(1))\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
      ],
      "metadata": {
        "id": "Vd3ldEsPghiG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "But the testing should be done on the testing dataset.\n",
        "Check to see that the number of events (number of input vectors) is what you expect.\n",
        "\n",
        "We'll also compare the loss value on the testing data to the last loss value on the training data."
      ],
      "metadata": {
        "id": "8OeJtclEl1hI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "# Get all predictions\n",
        "model.eval()\n",
        "all_predictions = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch_features, batch_labels in testing_dataloader:\n",
        "        outputs = model(batch_features)\n",
        "        predictions = (outputs > 0.5).float()\n",
        "        loss = criterion(predictions, batch_labels.unsqueeze(1))\n",
        "        all_predictions.extend(predictions.numpy())\n",
        "        all_labels.extend(batch_labels.numpy())\n",
        "\n",
        "print(loss)\n",
        "\n",
        "# Create confusion matrix\n",
        "cm = confusion_matrix(all_labels, all_predictions)\n",
        "\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.ylabel('True Label')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pttk11s2gnpw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's try the decision tree, as implemented in scikit-learn.\n",
        "We don't need the PyTorch dataloader here, just the training and testing datasets.\n",
        "Unfortunately the training and testing datasets were made as Torch tensors, and we need simple"
      ],
      "metadata": {
        "id": "Y8koEtvcOuVZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_dataset[0])\n",
        "print(test_dataset[0])"
      ],
      "metadata": {
        "id": "SRvNvdnIinIT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68212332"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# Prepare training data\n",
        "train_features_list = []\n",
        "train_labels_list = []\n",
        "for features, label in train_dataset:\n",
        "    train_features_list.append(features)\n",
        "    train_labels_list.append(label)\n",
        "\n",
        "X_train = torch.stack(train_features_list).numpy()\n",
        "y_train = torch.stack(train_labels_list).numpy()\n",
        "\n",
        "# Prepare testing data\n",
        "test_features_list = []\n",
        "test_labels_list = []\n",
        "for features, label in test_dataset:\n",
        "    test_features_list.append(features)\n",
        "    test_labels_list.append(label)\n",
        "\n",
        "X_test = torch.stack(test_features_list).numpy()\n",
        "y_test = torch.stack(test_labels_list).numpy()\n",
        "\n",
        "print(f\"Shape of X_train: {X_train.shape}\")\n",
        "print(f\"Shape of y_train: {y_train.shape}\")\n",
        "print(f\"Shape of X_test: {X_test.shape}\")\n",
        "print(f\"Shape of y_test: {y_test.shape}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier, GradientBoostingClassifier,AdaBoostClassifier\n",
        "from sklearn import datasets # import inbuild datasets\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics\n",
        "\n",
        "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "score=[]\n",
        "dtclassifier = DecisionTreeClassifier()\n",
        "dtclassifier.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "I7yYXbFOiN4d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dtclassifier.score(X_train,y_train),dtclassifier.score(X_test,y_test)"
      ],
      "metadata": {
        "id": "wcZC2c2vn4e3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "# Get predictions from the DecisionTreeClassifier on the test set\n",
        "y_pred_dt = dtclassifier.predict(X_test)\n",
        "\n",
        "# Create confusion matrix for Decision Tree\n",
        "cm_dt = confusion_matrix(y_test, y_pred_dt)\n",
        "\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(cm_dt, annot=True, fmt='d', cmap='Blues')\n",
        "plt.title('Confusion Matrix for Decision Tree Classifier')\n",
        "plt.ylabel('True Label')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6NwK6uwEoG9t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, let's try the XGBoost and see if the decision tree can be improved.\n",
        "\n",
        "Fortunately XGBoost is also part of scikit-learn, so we can use the same training and testing dataset formats as for the DecisionTreeClassifier."
      ],
      "metadata": {
        "id": "Er2CCC4uk8Rm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBClassifier\n",
        "import xgboost\n",
        "xgbclf = XGBClassifier(n_estimators=2, max_depth=2, learning_rate=1, objective='binary:logistic')\n",
        "# fit model\n",
        "xgbclf.fit(X_train, y_train)\n",
        "# make predictions\n",
        "preds = xgbclf.predict(X_test)"
      ],
      "metadata": {
        "id": "0lYyqu10k7bc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get predictions from the DecisionTreeClassifier on the test set\n",
        "y_pred_dt = xgbclf.predict(X_test)\n",
        "\n",
        "# Create confusion matrix for Decision Tree\n",
        "cm_dt = confusion_matrix(y_test, y_pred_dt)\n",
        "\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(cm_dt, annot=True, fmt='d', cmap='Blues')\n",
        "plt.title('Confusion Matrix for Decision Tree Classifier')\n",
        "plt.ylabel('True Label')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pp30QjKflDdv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusions\n",
        "\n",
        "Are the confusion matrices detailed enough to help us understand the performance of the various machine learning methods?\n",
        "\n",
        "They help us calculate a \"false positive rate\" and a \"true positive rate\" for each ML algorithm.\n",
        "We can explore different working points for each algorithm by changing the tree queries.\n",
        "\n",
        "Here is an example of the \"Receiver Operating Curve\" that plots the tradeoff between \"false positive rate\" and a \"true positive rate\" on the same plot.\n",
        "\n",
        "Can you guess which point on the curve represents the best overall performance? [Hint: the dotted blue line is a random classifier (a coin flip).]"
      ],
      "metadata": {
        "id": "8Lp1kKEJoxQg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Get predicted probabilities for the positive class\n",
        "y_pred_proba_xgb = xgbclf.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calculate ROC curve\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba_xgb)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "# Plot ROC curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Classifier')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve for XGBoost Classifier')\n",
        "plt.legend(loc='lower right')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6wyzFLa5ocVc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}