{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Convolutional Neural Networks with the MNIST handwriting dataset\n",
        "\n",
        "Deep Neural Networks can have exceedingly large number of parameters, making them difficult to train and stabilize.\n",
        "\n",
        "In the case of image data, there is no reason every pixel has to be connected to every other pixel. There are very good reasons for reducing the connectivity and for pooling information from groups of pixels.\n",
        "\n",
        "In this notebook we will introduce a Convolutional Neural Network trained to recognize and classify handwritten digits from the MNIST dataset.\n",
        "\n",
        "We'll also continue to evolve the \"standard\" PyTorch approach for setting up CNNs (or other complex DNNs) and training on large datasets."
      ],
      "metadata": {
        "id": "1YHujAgrNnWy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the MNIST dataset\n",
        "\n",
        "As always, we start by checking the data.\n",
        "In this case, we are loading the MNIST dataset as pixelated data points, without any transformation, that is, without any rescaling and renormalizing of individual pixel values."
      ],
      "metadata": {
        "id": "Kheawhb_duC4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import datasets, transforms\n",
        "\n",
        "# For visualization: no transform needed\n",
        "vis_dataset = datasets.MNIST('./data', train=True, download=True, transform=None)\n"
      ],
      "metadata": {
        "id": "Dy7sTpbHOok-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plot example data for checks\n",
        "\n",
        "The pixel intensity values can be interpreted on a grayscale color map. These are 28 x 28 pixel maps."
      ],
      "metadata": {
        "id": "oud3dykePrsy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create a figure with subplots\n",
        "fig, axes = plt.subplots(2, 5, figsize=(12, 6))\n",
        "fig.suptitle('MNIST Dataset Examples', fontsize=16)\n",
        "\n",
        "# Display 10 random samples\n",
        "for i, ax in enumerate(axes.flat):\n",
        "    # Get a random image and its label\n",
        "    img, label = vis_dataset[i]\n",
        "\n",
        "    # Display the image\n",
        "    ax.imshow(img, cmap='gray')\n",
        "    ax.set_title(f'Label: {label}')\n",
        "    ax.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "l3DOdVJoOZDf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define network model\n",
        "\n",
        "For the first time, we implement the NN model in a separate class, made up of an initialization function and organizational function. We have got away without such a class with the simple feedforward networks, but using a class is the best practice with PyTorch, and we will need the class for more complex models.\n",
        "\n",
        "- convolutional layers\n",
        "- pooling layer\n",
        "- fully connected layers\n",
        "\n",
        "Note that the flattening happens between the last pooling layer and the first fully connected layer."
      ],
      "metadata": {
        "id": "lUoTl79beYQM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define the CNN architecture\n",
        "# Best practice with PyTorch is to wrap this in a class.\n",
        "# (It doesn't matter for feedforward networks, but it will matter\n",
        "#  for more complicated networks.)\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        # This function just defines the layers but does not place them.\n",
        "        # First convolutional layer: 1 input channel (grayscale), 32 output channels, 3x3 kernel\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
        "        # Max pooling layer: 2x2 window\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        # Second convolutional layer: 32 input channels, 64 output channels, 3x3 kernel\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        # Fully connected layers\n",
        "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "        # Activation function generic description\n",
        "        # The ReLU layers will be used in the forward function (below)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # This function places the layers that were defined above.\n",
        "        # First conv block: conv -> relu -> pool\n",
        "        x = self.pool(self.relu(self.conv1(x)))  # 28x28 -> 14x14\n",
        "        # Second conv block: conv -> relu -> pool\n",
        "        x = self.pool(self.relu(self.conv2(x)))  # 14x14 -> 7x7\n",
        "        # Flatten for fully connected layers\n",
        "        x = x.view(-1, 64 * 7 * 7)\n",
        "        # First fully connected layer with ReLU\n",
        "        x = self.relu(self.fc1(x))\n",
        "        # Output layer (no activation, will use CrossEntropyLoss)\n",
        "        x = self.fc2(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "mih1yXK-OF-a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set up training and testing\n",
        "\n",
        "In this way, we will set up the train and test functions as definitions, so that we can easily call them over and over.\n",
        "Our loss function is the CrossEntropyLoss (why?), and we are taking the output with the maximum value as our prediction."
      ],
      "metadata": {
        "id": "fEXXOy9PhPX7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training function\n",
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = nn.CrossEntropyLoss()(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch_idx % 100 == 0:\n",
        "            print(f'Training Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)}] Loss: {loss.item():.6f}')\n",
        "\n",
        "# Testing function\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += nn.CrossEntropyLoss()(output, target).item()\n",
        "            pred = output.argmax(dim=1, keepdim=True)\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader)\n",
        "    accuracy = 100. * correct / len(test_loader.dataset)\n",
        "    print(f'\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} ({accuracy:.2f}%)\\n')\n"
      ],
      "metadata": {
        "id": "KzJjRpm3QBZ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##"
      ],
      "metadata": {
        "id": "g39tDCC2kli5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading and training\n",
        "\n",
        "There are 3 things of note here:\n",
        "- We are using the GPU if it is available. This will greatly speed up the training.\n",
        "- The training and testing data are transformed through rescaling so that the values are converted to a convenient range for the input layer. We did not have to worry about this for the visualization of the data.\n",
        "- The batch size is larger than we have used in the past. Of course the batch size for the testing data does not matter at all. (Does it?)"
      ],
      "metadata": {
        "id": "p4718447vPLn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Using device: {device}')\n",
        "\n",
        "# For training: full transform pipeline that scales inputs 0-1 and normalizes\n",
        "transform = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.1307,), (0.3081,))  # MNIST mean and std\n",
        "            ])\n",
        "\n",
        "# Load MNIST dataset\n",
        "train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.MNIST('./data', train=False, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)\n",
        "\n",
        "# Create and load model\n",
        "model = SimpleCNN().to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Train for 5 epochs\n",
        "for epoch in range(1, 3):\n",
        "  train(model, device, train_loader, optimizer, epoch)\n",
        "  test(model, device, test_loader)\n",
        "\n",
        "\n",
        "# Save the model\n",
        "torch.save(model.state_dict(), 'mnist_cnn.pth')\n",
        "print('Model saved to mnist_cnn.pth')"
      ],
      "metadata": {
        "id": "TaJ07Fk8RHe_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Saving the model (CNN)\n",
        "\n",
        "The last line saves the model (trained with many GPU cycles and energy units!). This can be useful for preserving the state of training, continuing training with more data, saving the model so that it can be used for prediction (aka inference).\n",
        "\n",
        "How many parameters does this model have?"
      ],
      "metadata": {
        "id": "xhnvBp5JwbCx"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iQVNNiuwRKVB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}