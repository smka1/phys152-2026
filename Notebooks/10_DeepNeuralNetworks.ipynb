{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Deep Neural Networks with the iris ML dataset\n",
        "\n",
        "We show how to use the special DNN techniques:\n",
        "- multi-class output\n",
        "- loss functions,\n",
        "- batch training,\n",
        "- 1cycle learning rate adjustment\n",
        "- input normalization\n",
        "\n",
        "Admittedly, this is overkill for the small number of input features and small data of the size iris dataset, but we will have fun!\n",
        "\n",
        "This also gives some idea of the \"standard\" PyTorch approach for setting up DNNs and training on large datasets."
      ],
      "metadata": {
        "id": "1YHujAgrNnWy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "iris = datasets.load_iris()\n",
        "print(iris.feature_names)\n",
        "print(iris.target_names)\n",
        "\n",
        "X, y = iris.data, iris.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,random_state=42)"
      ],
      "metadata": {
        "id": "qdUHeL93KMRg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is no reason to expect the input features to be in the range (-1,1), but this is where our activation functions are most powerful.\n",
        "\n",
        "Scaling the features also helps ensure that different input features are treated on the same footing, whether their numerical values are large or small.\n",
        "\n",
        "The most challenging part of the setup is pushing all of the data into the PyTorch tensors for training and testing. Just use some standard `DataLoader` code to accomplish that.\n",
        "\n",
        "For the first time, we introduce (mini-)batches that are a fraction of the total training dataset. This allows us to load an entire training batch into the GPU memory at once (even if we're not yet using GPUs). The noise from the mini-batches also helps with the generalization to testing data. A typical batch size is 16-128, depending on the dataset and model."
      ],
      "metadata": {
        "id": "82juJRR8OR9n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# normalize the input features to be within (0,1)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# convert to PyTorch tensors\n",
        "X_train = torch.FloatTensor(X_train)\n",
        "y_train = torch.LongTensor(y_train)\n",
        "X_test = torch.FloatTensor(X_test)\n",
        "y_test = torch.LongTensor(y_test)\n",
        "\n",
        "# create TensorDataset\n",
        "train_dataset = TensorDataset(X_train, y_train)\n",
        "test_dataset = TensorDataset(X_test, y_test)\n",
        "\n",
        "# create PyTorch DataLoader with batches for training\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=16,      # Adjust based on your needs\n",
        "    shuffle=True        # Shuffle training data in batches\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=16,\n",
        "    shuffle=False       # Don't shuffle test data\n",
        ")\n",
        "\n",
        "print(f\"Training samples: {len(train_dataset)}\")\n",
        "print(f\"Test samples: {len(test_dataset)}\")\n",
        "print(f\"Batches per epoch: {len(train_loader)}\")\n"
      ],
      "metadata": {
        "id": "2KXgbQSWL9GS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that each epoch will include 7 batches, so the model weights will be updated 7 times per epoch.\n",
        "\n",
        "Now let's create the DNN model itself. We use the ReLU function. We also introduce a new loss function and learning rate scheduler tuned for DNN multi-class classification."
      ],
      "metadata": {
        "id": "LlIdI6y91u05"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N3UdGPms8w11"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import OneCycleLR\n",
        "\n",
        "# define DNN model\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(4, 10),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(10, 10),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(10, 10),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(10, 3)\n",
        ")\n",
        "\n",
        "# CrossEntropyLoss made for multi-class classification\n",
        "# It combines LogSoftMax and NLLLoss internally\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# SGD with momentum\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
        "\n",
        "# 1cycle implement in Torch as OneCycleLR\n",
        "scheduler = OneCycleLR(\n",
        "    optimizer,\n",
        "    max_lr=0.1,                      # Peak learning rate\n",
        "    epochs=100,                      # Total epochs\n",
        "    steps_per_epoch=len(train_loader),  # Batches per epoch\n",
        "    pct_start=0.3,                   # 30% warmup, 70% annealing\n",
        "    anneal_strategy='cos',           # Cosine annealing\n",
        "    div_factor=25.0,                 # Initial LR = max_lr/25\n",
        "    final_div_factor=1e4             # Final LR = max_lr/10000\n",
        ")\n",
        "\n",
        "# training loop\n",
        "for epoch in range(100):\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # CRITICAL for 1cycle: step scheduler after each batch, not each epoch!\n",
        "        scheduler.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The training is very fast! Why do you think that is the case?\n",
        "\n",
        "Now let's evaluate the performance of the model. We can turn off the gradient tracking for the evaluation.\n",
        "\n",
        "Instead of simply asking whether the model got the classification correct, I want to know the accuracy for each one of the target classes individually."
      ],
      "metadata": {
        "id": "JAlbDwul2H4C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval() # Set the model to evaluation mode\n",
        "\n",
        "# Initialize variables for overall accuracy\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "# Initialize variables for per-class accuracy\n",
        "num_classes = len(iris.target_names)\n",
        "correct_pred = list(0. for i in range(num_classes))\n",
        "total_pred = list(0. for i in range(num_classes))\n",
        "\n",
        "with torch.no_grad(): # Disable gradient calculation during inference\n",
        "    for data, target in test_loader:\n",
        "        outputs = model(data)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "        # Overall accuracy\n",
        "        total += target.size(0)\n",
        "        correct += (predicted == target).sum().item()\n",
        "\n",
        "        # Per-class accuracy\n",
        "        for label in range(num_classes):\n",
        "            label_mask = (target == label)\n",
        "            total_pred[label] += label_mask.sum().item()\n",
        "            correct_pred[label] += ((predicted == label) & label_mask).sum().item()\n",
        "\n",
        "overall_accuracy = 100 * correct / total\n",
        "print(f'Overall Accuracy of the model on the test data: {overall_accuracy:.2f}%')\n",
        "\n",
        "print('\\nAccuracy for each class:')\n",
        "for i, class_name in enumerate(iris.target_names):\n",
        "    if total_pred[i] > 0:\n",
        "        class_accuracy = 100 * correct_pred[i] / total_pred[i]\n",
        "        print(f'  {class_name}: {class_accuracy:.2f}%')\n",
        "    else:\n",
        "        print(f'  {class_name}: No samples in test set')"
      ],
      "metadata": {
        "id": "rn2IhAQZMlsB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Does this result look reasonable to you? Are the results what you expected from the DNN?"
      ],
      "metadata": {
        "id": "P3JVjDCn2ap2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Or try this model with ELU\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(4, 10),\n",
        "    nn.ELU(alpha=1.0),  # alpha is optional, default is 1.0\n",
        "    nn.Linear(10, 10),\n",
        "    nn.ELU(alpha=1.0),\n",
        "    nn.Linear(10, 10),\n",
        "    nn.ELU(alpha=1.0),\n",
        "    nn.Linear(10, 3)\n",
        ")"
      ],
      "metadata": {
        "id": "5kKiQKMCNgks"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}